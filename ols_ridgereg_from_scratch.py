# -*- coding: utf-8 -*-
"""OLS_RidgeReg_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xcjCj8j3aylBQkJI7Ty5L80-3GmJI4po
"""

import numpy as np

class RidgeReg:
  def __init__(self, alph=0.01):
    self.alpha=alph #hyperparameter: Default value is set to be 0.01
    self.X=None
  def train(self,X,Y):
    self.Y=Y
    cols=X.shape[1]
    new_X=np.insert(X,0,1,axis=1) #iserting 1 in first column of X train data
    self.I=np.identity(cols+1) #making an identity matrix of no.features size
    self.I[0,0]=0 #To not regularize the bias term. Cause in Ridge reg, only slope/coefs are regularized, as they represent high dependency of Y on X

    #boring caculation: B=(XT+alpha*I)XT.Y
    term_1=np.dot(new_X.T,new_X)+self.alpha*self.I
    term_2=np.linalg.inv(term_1)
    term_3=np.dot(new_X.T,Y)
    self.B=np.dot(term_2,term_3)
  def predict(self,X_test):
    new_X_test=np.insert(X_test,0,1,axis=1)
    return np.dot(new_X_test,self.B)

from sklearn.datasets import load_diabetes

x,y=load_diabetes(return_X_y=True)
print(x.shape,x,"\n",y.shape,y)

import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=12)

model = Ridge(alpha=0.01)
model.fit(x_train,y_train)

from sklearn.metrics import r2_score

model.intercept_

model.coef_

r2_score(y_test,model.predict(x_test))

Ridge=RidgeReg()

Ridge.train(x_train,y_train)

Ridge.B

y_pred=Ridge.predict(x_test)
y_pred

r2_score(y_test,y_pred)

#My RidgeReg and sklearns Ridge class gave the exact same result. 38.301478777142206% of variance is explained.